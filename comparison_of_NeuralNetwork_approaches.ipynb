{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "comparison_automated.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMVpv8zxO1waASbEEDr0aJ6"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG9_OUV01_AL"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RULpry8O1x8t"
      },
      "source": [
        "import math\n",
        "import time \n",
        "import numpy as np\n",
        "import sympy as sym\n",
        "import scipy.special\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import *\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "from tensorflow.keras.applications.vgg16 import VGG16"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BiDNpqk2JoN"
      },
      "source": [
        "def check_gpu():\n",
        "  %tensorflow_version 2.x\n",
        "  device_name = tf.test.gpu_device_name()\n",
        "  if device_name != '/device:GPU:0':\n",
        "    # raise SystemError('GPU device not found')\n",
        "    return False\n",
        "  else:\n",
        "    print('Found GPU at: {}'.format(device_name)) \n",
        "    return True\n",
        "\n",
        "def conv_to_latex(data_acc):\n",
        "  latex_data_points_batch_acc = ''\n",
        "  latex_data_points_time_batch = ''\n",
        "  for i in data_acc:\n",
        "    latex_data_points_batch_acc += f' {(i[0], i[1])}'\n",
        "    latex_data_points_time_batch += f' {round(i[2]/60, 2), i[0]}'\n",
        "  return(latex_data_points_batch_acc, latex_data_points_time_batch)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ln8H2ZA2wvn"
      },
      "source": [
        "## Data sets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVTLLGkT4CKK",
        "outputId": "0fa00d87-5614-45d2-ec8e-177338c83b4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKgwL-mr329d"
      },
      "source": [
        "### Mnist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVD80xwh2z01",
        "outputId": "27cc3227-9ab6-42e2-9e06-b0f3f5860066",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "(x_test, y_test), (x_train, y_train) = mnist.load_data()\n",
        "\n",
        "x_train, x_test = x_train/255.0, x_test/255.0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUzSb64C363o"
      },
      "source": [
        "### EuroSat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haTTI59R39cb"
      },
      "source": [
        "pickle_in = open(\"./gdrive/My Drive/Data/EuroSat/X_euroSat.pickle\", \"rb\")\n",
        "X = pickle.load(pickle_in)\n",
        "\n",
        "pickle_in = open(\"./gdrive/My Drive/Data/EuroSat/Y_euroSat.pickle\", \"rb\")\n",
        "Y = pickle.load(pickle_in)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyrO4e28QJxc"
      },
      "source": [
        "val_split = int(round(len(X)*0.8))\n",
        "X_train, Y_train = X[:val_split], Y[:val_split] \n",
        "X_test, Y_test = X[val_split:], Y[val_split:]\n",
        "\n",
        "x_train = np.array(X_train[:10000])\n",
        "y_train = np.array(Y_train[:10000])\n",
        "\n",
        "x_test = np.array(X_test[:2000])\n",
        "y_test = np.array(Y_test[:2000])\n",
        "\n",
        "x_train, x_test = x_train/255.0, x_test/255.0"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vPldKjr3GdJ"
      },
      "source": [
        "## From scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuG_tZwO3swG"
      },
      "source": [
        "### Vanilla NN from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6GMQjZA3OMY"
      },
      "source": [
        "class NeuralNetwork:\n",
        "    class Architecture:\n",
        "        class ArchitectureLayer:\n",
        "            def __init__(self, nodes, activation_function):\n",
        "                self.__nodes = nodes\n",
        "                self.__af = activation_function\n",
        "\n",
        "            def call_activation_function(self, x):\n",
        "                if self.__af == \"sigmoid\":\n",
        "                    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "            def __get_nodes(self):\n",
        "                return self.__nodes\n",
        "\n",
        "            nodes = property(__get_nodes)\n",
        "\n",
        "        class ArchitectureNetwork:\n",
        "            def __init__(self, layers, learning_rate):\n",
        "                self.__layers = layers\n",
        "                self.__lr = learning_rate\n",
        "                self.__w = [np.random.rand(self.__layers[layer + 1].nodes, self.__layers[layer].nodes) - 0.5\n",
        "                            for layer in range(len(self.__layers) - 1)]\n",
        "\n",
        "            def __get_weights(self):\n",
        "                return self.__w\n",
        "\n",
        "            def __get_layers(self):\n",
        "                return self.__layers\n",
        "\n",
        "            def __get_learning_rate(self):\n",
        "                return self.__lr\n",
        "\n",
        "            def __set_weights(self, w):\n",
        "                self.__w = w\n",
        "\n",
        "            layers = property(__get_layers)\n",
        "            lr = property(__get_learning_rate)\n",
        "            w = property(__get_weights, __set_weights)\n",
        "\n",
        "    class Training:\n",
        "        def __init__(self, model):\n",
        "            self.__model = model\n",
        "\n",
        "        def train(self, train_images, train_labels):\n",
        "            for image, label in zip(train_images, train_labels):\n",
        "                image = image * 0.99 + 0.1\n",
        "                targets = np.zeros(self.__model.layers[-1].nodes) + 0.1\n",
        "                targets[label] = 0.99\n",
        "                inputs = np.array(image.flatten(), ndmin=2).T\n",
        "                targets = np.array(targets, ndmin=2).T\n",
        "                inputs_array = [inputs]\n",
        "                for i in range(len(self.__model.layers) - 1):\n",
        "                    signal = np.dot(self.__model.w[i], inputs)\n",
        "                    inputs = self.__model.layers[i].call_activation_function(signal)\n",
        "                    inputs_array.append(inputs)\n",
        "                error_output = targets - inputs\n",
        "                for k in range(len(self.__model.layers)-2, -1, -1):\n",
        "                    self.__model.w[k] += self.__model.lr * np.dot((error_output *\n",
        "                                        inputs_array[k+1] * (1 - inputs_array[k+1])), np.transpose(inputs_array[k]))\n",
        "                    error_output = np.dot(self.__model.w[k].T, error_output)\n",
        "            return self.__model\n",
        "\n",
        "        def epoch_training(self, train_images, train_labels, epochs, test_images, test_labels):\n",
        "            accuracies = []\n",
        "            for epoch in range(epochs):\n",
        "                # print(f'Epoch {epoch+1}/{epochs}')\n",
        "                shuffle_data = np.random.permutation(len(train_images))\n",
        "                train_images = train_images[shuffle_data]\n",
        "                train_labels = train_labels[shuffle_data]\n",
        "                NeuralNetwork.Training.train(NeuralNetwork.Training(self.__model), train_images, train_labels)\n",
        "                val_accuracy = NeuralNetwork.Evaluation.evaluate(NeuralNetwork.Evaluation(self.__model), test_images, test_labels)\n",
        "                accuracies.append(val_accuracy)\n",
        "                # print(f'Accuracy: {val_accuracy}')\n",
        "            return self.__model, accuracies\n",
        "\n",
        "    class Evaluation:\n",
        "        def __init__(self, model):\n",
        "            self.__model = model\n",
        "\n",
        "        def evaluate(self, test_images, test_labels):\n",
        "            accuracy = 0\n",
        "            for image, label in zip(test_images, test_labels):\n",
        "                image = image * 0.99 + 0.1\n",
        "                inputs = np.array(image.flatten(), ndmin=2).T\n",
        "                for i in range(len(self.__model.layers) - 1):\n",
        "                    signal = np.dot(self.__model.w[i], inputs)\n",
        "                    inputs = self.__model.layers[i].call_activation_function(signal)\n",
        "                inputs = np.argmax(inputs)\n",
        "                if inputs == label:\n",
        "                    accuracy += 1\n",
        "            return accuracy / len(test_images)\n",
        "        \n",
        "        "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQUqvTo53YRK"
      },
      "source": [
        "### CNN from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qHZmrtx3c1d"
      },
      "source": [
        "class ConvolutionalNeuralNetwork:  # NeuralNetwork\n",
        "    class Conv_op:\n",
        "        def __init__(self, num_filters, filter_size, channels=3):\n",
        "            self.__num_filters = num_filters\n",
        "            self.__filter_size = filter_size\n",
        "            self.__channels = channels\n",
        "            self.__conv_filter = np.random.randn(self.__num_filters, self.__channels, filter_size, filter_size) \\\n",
        "                               / (filter_size * filter_size)\n",
        "\n",
        "        def image_region(self, image):\n",
        "            height, width = image.shape[:2]  \n",
        "            self.__image = image\n",
        "            for j in range(height - self.__filter_size + 1):\n",
        "                for k in range(width - self.__filter_size + 1):\n",
        "                    if self.__channels > 1:\n",
        "                        image_patch = image[j:(j + self.__filter_size), k:(k + self.__filter_size), :]\n",
        "                        yield image_patch, j, k\n",
        "                    else:\n",
        "                        image_patch = image[j:(j + self.__filter_size), k:(k + self.__filter_size)]\n",
        "                        yield image_patch, j, k\n",
        "\n",
        "        def forward_prop(self, image):\n",
        "            height, width = image.shape[:2]\n",
        "            conv_out = np.zeros((height - self.__filter_size + 1, width - self.__filter_size + 1, self.__num_filters))\n",
        "            for image_patch, i, j in self.image_region(image):\n",
        "                conv_out[i, j] = np.sum(image_patch * self.__conv_filter, axis=(1, 2, 3))\n",
        "            return conv_out\n",
        "\n",
        "        def back_prop(self, dL_dout, learning_rate):\n",
        "            dL_dF_params = np.zeros(self.__conv_filter.shape)\n",
        "            for image_patch, i, j in self.image_region(self.__image):\n",
        "                for k in range(self.__num_filters):\n",
        "                    dL_dF_params[k] += image_patch * dL_dout[i, j, k]\n",
        "            self.__conv_filter -= learning_rate * dL_dF_params\n",
        "            return dL_dF_params\n",
        "\n",
        "    class Max_Pool:\n",
        "        def __init__(self, filter_size):\n",
        "            self.__filter_size = filter_size\n",
        "\n",
        "        def image_region(self, image):\n",
        "            new_height = image.shape[0] // self.__filter_size\n",
        "            new_width = image.shape[1] // self.__filter_size\n",
        "            self.__image = image\n",
        "            for i in range(new_height):\n",
        "                for j in range(new_width):\n",
        "                    image_patch = image[(i * self.__filter_size): (i * self.__filter_size + self.__filter_size),\n",
        "                                  (j * self.__filter_size):(j * self.__filter_size + self.__filter_size)]\n",
        "                    yield image_patch, i, j\n",
        "\n",
        "        def forward_prop(self, image):\n",
        "            height, width, num_filters = image.shape\n",
        "            output = np.zeros((height // self.__filter_size, width // self.__filter_size, num_filters))\n",
        "            for image_patch, i, j in self.image_region(image):\n",
        "                output[i, j] = np.amax(image_patch, axis=(0, 1))\n",
        "            return output\n",
        "\n",
        "        def back_prop(self, dL_dout):\n",
        "            dL_dmax_pool = np.zeros(self.__image.shape)\n",
        "            for image_patch, i, j in self.image_region(self.__image):\n",
        "                height, width, num_filters = image_patch.shape\n",
        "                maximum_val = np.amax(image_patch, axis=(0, 1))\n",
        "                for i1 in range(height):\n",
        "                    for j1 in range(width):\n",
        "                        for k1 in range(num_filters):\n",
        "                            if image_patch[i1, j1, k1] == maximum_val[k1]:\n",
        "                                dL_dmax_pool[i * self.__filter_size + i1, j * self.__filter_size + j1, k1] = dL_dout[i, j, k1]\n",
        "                return dL_dmax_pool\n",
        "\n",
        "    class Softmax:\n",
        "        def __init__(self, input_node, softmax_node):\n",
        "            self.__weights = np.random.randn(input_node, softmax_node) / input_node\n",
        "            self.__bias = np.zeros(softmax_node)\n",
        "            self.__softmax_nodes = softmax_node\n",
        "\n",
        "        def forward_prop(self, image):\n",
        "            self.__orig_im_shape = image.shape   \n",
        "            image_modified = image.flatten()\n",
        "            self.__modified_imput = image_modified \n",
        "            output_val = np.dot(image_modified, self.__weights) + self.__bias\n",
        "            self.__out = output_val    \n",
        "            exp_out = np.exp(output_val)\n",
        "            return exp_out / np.sum(exp_out, axis=0)\n",
        "\n",
        "        def back_prop(self, dL_dout, learning_rate):\n",
        "            for i, grad in enumerate(dL_dout):\n",
        "                if grad == 0:\n",
        "                    continue\n",
        "                transformation_eq = np.exp(self.__out)\n",
        "                S_total = np.sum(transformation_eq)\n",
        "                dy_dz = -transformation_eq[i] * transformation_eq / (S_total ** 2)\n",
        "                dy_dz[i] = transformation_eq[i] * (S_total - transformation_eq[i]) / (S_total ** 2)\n",
        "                dz_dw = self.__modified_imput\n",
        "                dz_db = 1\n",
        "                dz_d_inp = self.__weights\n",
        "                dL_dz = grad * dy_dz\n",
        "                dL_dw = dz_dw[np.newaxis].T @ dL_dz[np.newaxis]\n",
        "                dL_db = dL_dz * dz_db\n",
        "                dL_d_inp = dz_d_inp @ dL_dz\n",
        "                self.__weights -= learning_rate * dL_dw\n",
        "                self.__bias -= learning_rate * dL_db\n",
        "                return dL_d_inp.reshape(self.__orig_im_shape)\n",
        "\n",
        "        def __get_softmax_nodes(self):\n",
        "            return self.__softmax_nodes\n",
        "\n",
        "        snodes = property(__get_softmax_nodes)\n",
        "\n",
        "    class Training:\n",
        "        def __init__(self, conv, pool, softmax):\n",
        "            self.__conv = conv\n",
        "            self.__pool = pool\n",
        "            self.__softmax = softmax\n",
        "\n",
        "        def cnn_forward_prop(self, image, label):\n",
        "            out_p = self.__conv.forward_prop(image - 0.5)\n",
        "            out_p = self.__pool.forward_prop(out_p)\n",
        "            out_p = self.__softmax.forward_prop(out_p)\n",
        "            cross_ent_loss = -np.log(out_p[label])\n",
        "            if np.argmax(out_p) == label:\n",
        "                accuracy_eval = 1\n",
        "            else:\n",
        "                accuracy_eval = 0\n",
        "            return out_p, cross_ent_loss, accuracy_eval\n",
        "\n",
        "        def train_cnn(self, image, label, learning_rate=.005):\n",
        "            out, loss, acc = self.cnn_forward_prop(image, label)\n",
        "            gradient = np.zeros(self.__softmax.snodes)\n",
        "            gradient[label] = -1 / out[label]\n",
        "            grad_back = self.__softmax.back_prop(gradient, learning_rate)\n",
        "            grad_back = self.__pool.back_prop(grad_back)\n",
        "            self.__conv.back_prop(grad_back, learning_rate)\n",
        "            return loss, acc\n",
        "\n",
        "        def epochs_training(self, epochs, train_images, train_labels, test_images, test_labels):\n",
        "            accuracies = []\n",
        "            model = ConvolutionalNeuralNetwork.Training(self.__conv, self.__pool, self.__softmax)  # NN\n",
        "            for epoch in range(epochs):\n",
        "                print(f'Epoch {epoch+1}/{epochs}')\n",
        "                shuffle_data = np.random.permutation(len(train_images))\n",
        "                train_images = train_images[shuffle_data]\n",
        "                train_labels = train_labels[shuffle_data]\n",
        "                loss = 0\n",
        "                num_correct = 0\n",
        "                for i, (im, label) in enumerate(zip(train_images, train_labels)):\n",
        "                    if i % 100 == 0:\n",
        "                        print(f'{int(i / 100)+1} steps out of {int(len(train_images)/100)} '\n",
        "                              f'steps: Accuracy: {num_correct / 100}')\n",
        "                        loss = 0\n",
        "                        num_correct = 0\n",
        "                    l, accu = self.train_cnn(im, label)\n",
        "                    loss += l\n",
        "                    num_correct += accu\n",
        "                accuracy = ConvolutionalNeuralNetwork.Evaluation.evaluate_cnn(ConvolutionalNeuralNetwork.Evaluation(model), test_images, test_labels)\n",
        "                accuracies.append(accuracy)\n",
        "            return accuracies\n",
        "\n",
        "    class Evaluation:\n",
        "        def __init__(self, model):\n",
        "            self.__model = model\n",
        "\n",
        "        def evaluate_cnn(self, test_images, test_labels):\n",
        "            loss = 0\n",
        "            num_correct = 0\n",
        "            for im, label in zip(test_images, test_labels):\n",
        "                _, l, accu = ConvolutionalNeuralNetwork.Training.cnn_forward_prop(self.__model, im, label)\n",
        "                loss += l\n",
        "                num_correct += accu\n",
        "            num_tests = len(test_images)\n",
        "            accuracy = num_correct / num_tests\n",
        "            # print('Test Loss:', loss / num_tests)\n",
        "            print('Test Accuracy:', accuracy)\n",
        "            return accuracy\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh4IoN5L4RB4"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lqhZmLRPqXM"
      },
      "source": [
        "### Approach Mnist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpiQUmUp4T-6"
      },
      "source": [
        "def v_nn_s(train_images, train_labels, test_images, test_labels):\n",
        "  print(f'Datasize: {batch[0]}')\n",
        "  layers = [NeuralNetwork.Architecture.ArchitectureLayer(784, \"sigmoid\"),\n",
        "          NeuralNetwork.Architecture.ArchitectureLayer(100, \"sigmoid\"),\n",
        "          NeuralNetwork.Architecture.ArchitectureLayer(10, \"sigmoid\")]\n",
        "\n",
        "  nn = NeuralNetwork.Architecture.ArchitectureNetwork(layers, 0.1)\n",
        "  nn, accuracies = NeuralNetwork.Training.epoch_training(NeuralNetwork.Training(nn), train_images, train_labels, epochs, test_images, test_labels)\n",
        "  return accuracies[epochs-1]\n",
        "\n",
        "def v_nn_tf(train_images, train_labels, test_images, test_labels):\n",
        "  print(f'Datasize: {batch[0]}')\n",
        "  model = tf.keras.models.Sequential([\n",
        "          tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "          tf.keras.layers.Dense(784, activation='sigmoid'),\n",
        "          tf.keras.layers.Dense(100, activation='sigmoid'),\n",
        "          # tf.keras.layers.Dropout(0.05),\n",
        "          tf.keras.layers.Dense(10, activation='sigmoid')\n",
        "          ])\n",
        "\n",
        "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(train_images, train_labels, epochs=epochs, validation_data=(test_images, test_labels))\n",
        "\n",
        "  test_loss, test_acc = model.evaluate(test_images,  test_labels)\n",
        "  return test_acc\n",
        " \n",
        "\n",
        "def cnn_s(train_images, train_labels, test_images, test_labels):\n",
        "  print(f'Datasize: {batch[0]}')\n",
        "  conv = ConvolutionalNeuralNetwork.Conv_op(4, 3, channels=1)        \n",
        "  pool = ConvolutionalNeuralNetwork.Max_Pool(2)                      \n",
        "  softmax = ConvolutionalNeuralNetwork.Softmax(13*13*4, 10)         \n",
        "\n",
        "  model = ConvolutionalNeuralNetwork.Training(conv, pool, softmax)\n",
        "  accuracies = ConvolutionalNeuralNetwork.Training.epochs_training(model, epochs, train_images, train_labels, test_images, test_labels)\n",
        "  return accuracies[epochs-1]\n",
        "\n",
        "def cnn_tf(train_images, train_labels, test_images, test_labels):\n",
        "  print(f'Datasize: {batch[0]}')\n",
        "  model = tf.keras.models.Sequential([\n",
        "          tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
        "          tf.keras.layers.Dense(784, activation='sigmoid'),\n",
        "          tf.keras.layers.Dense(100, activation='sigmoid'),\n",
        "          # tf.keras.layers.Dropout(0.05),\n",
        "          tf.keras.layers.Dense(10, activation='sigmoid')\n",
        "          ])\n",
        "\n",
        "  model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(train_images, train_labels, epochs=epochs, validation_data=(test_images, test_labels))\n",
        "  test_loss, test_acc = model.evaluate(test_images,  test_labels)\n",
        "  return test_acc\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3oY7_04rF7l"
      },
      "source": [
        "### Approach EuroSat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvdLgrgHrEx6"
      },
      "source": [
        "def cnn_s(train_images, train_labels, test_images, test_labels):\n",
        "  print('')\n",
        "  print(f'Datasize: {batch[0]}')\n",
        "  conv = ConvolutionalNeuralNetwork.Conv_op(4, 3, channels=3)        \n",
        "  pool = ConvolutionalNeuralNetwork.Max_Pool(2)                      \n",
        "  softmax = ConvolutionalNeuralNetwork.Softmax(39*39*4, 10)          \n",
        "\n",
        "  model = ConvolutionalNeuralNetwork.Training(conv, pool, softmax)\n",
        "  accuracies = ConvolutionalNeuralNetwork.Training.epochs_training(model, epochs, train_images, train_labels, test_images, test_labels)\n",
        "  return accuracies[epochs-1]\n",
        "\n",
        "def cnn_tf(train_images, train_labels, test_images, test_labels):\n",
        "  print(f'Datasize: {batch[0]}')\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Conv2D(4, (3, 3), activation='relu', input_shape=(80, 80, 3)))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(10, activation='sigmoid'))\n",
        "\n",
        "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(train_images, train_labels, epochs=20, validation_split=0.2)\n",
        "  test_loss, test_acc = model.evaluate(test_images,  test_labels)\n",
        "  return test_acc\n",
        "\n",
        "def p_nn(train_images, train_labels, test_images, test_labels):\n",
        "  print(f'Datasize: {batch[0]}')\n",
        "  model = models.Sequential()\n",
        "  base_model = VGG16(input_shape = (80, 80, 3), include_top = False, weights = 'imagenet')\n",
        "  model.add(base_model)\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(10, activation='sigmoid'))\n",
        "  base_model.trainable = False\n",
        "\n",
        "  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(train_images, train_labels, epochs=20,validation_split=0.2) \n",
        "  test_loss, test_acc = model.evaluate(test_images,  test_labels)\n",
        "  return test_acc \n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndpdJETz84zW"
      },
      "source": [
        "### Optimized Models "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pst8YIW9Dzy"
      },
      "source": [
        "def cnn_tf(train_images, train_labels, test_images, test_labels):\n",
        "  print('')\n",
        "  print(f'Datasize: {batch[0]}')\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(80, 80, 3)))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(512, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(layers.Dense(10, activation='sigmoid'))\n",
        "\n",
        "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(train_images, train_labels, epochs=30, validation_split=0.2)\n",
        "  test_loss, test_acc = model.evaluate(test_images,  test_labels)\n",
        "  return test_acc\n",
        "\n",
        "def p_nn(train_images, train_labels, test_images, test_labels):\n",
        "  print('')\n",
        "  print(f'Datasize: {batch[0]}')\n",
        "  model = models.Sequential()\n",
        "  base_model = VGG16(input_shape = (80, 80, 3), include_top = False, weights = 'imagenet')\n",
        "  model.add(base_model)\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(256, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(layers.Dense(10, activation='sigmoid'))\n",
        "  base_model.trainable = False\n",
        "\n",
        "  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(train_images, train_labels, epochs=30, validation_split=0.2) \n",
        "  test_loss, test_acc = model.evaluate(test_images,  test_labels)\n",
        "  return test_acc "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp1aBn7u85CY"
      },
      "source": [
        "### Function calles "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOjJiZyGVML_"
      },
      "source": [
        "def call_func(train_images, train_labels, test_images, test_labels, train_images_r, test_images_r):\n",
        "  if func == 'v_nn_s':\n",
        "    accuracies = v_nn_s(train_images, train_labels, test_images, test_labels)\n",
        "  elif func == 'v_nn_tf':\n",
        "    accuracies = v_nn_tf(train_images, train_labels, test_images, test_labels)\n",
        "  elif func == 'cnn_s':\n",
        "    accuracies = cnn_s(train_images, train_labels, test_images, test_labels)\n",
        "  elif func == 'cnn_tf':\n",
        "    accuracies = cnn_tf(train_images_r, train_labels, test_images_r, test_labels)\n",
        "  elif func == 'p_nn':\n",
        "    accuracies = p_nn(train_images, train_labels, test_images, test_labels)\n",
        "  return accuracies\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXJ6c8fYPtf4"
      },
      "source": [
        "### Testing Mnist "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtAONe3U5fhA"
      },
      "source": [
        "data = [10, 50, 100, 500, 1000, 5000, 10000]\n",
        "data_acc = [[elem] for elem in data]\n",
        "approach_list = ['cnn_s'] # ['v_nn_s', 'v_nn_tf', 'cnn_s', 'cnn_tf'] # 'v_nn_s', 'v_nn_tf', 'cnn_s', 'cnn_tf', 'p_nn'\n",
        "\n",
        "latex_data = []\n",
        "epochs = 5\n",
        "r = 5  # robustness\n",
        "\n",
        "gpu = check_gpu()\n",
        "\n",
        "for func in approach_list:\n",
        "  for _ in range(r):\n",
        "    for batch in data_acc:\n",
        "      print('')\n",
        "      print(f'Datasize: {batch[0]}, Repetition: {_+1}')\n",
        "      train_images = x_train[:batch[0]]\n",
        "      train_labels = y_train[:batch[0]]\n",
        "      test_images = x_test[:int(batch[0]*0.2)]\n",
        "      test_labels = y_test[:int(batch[0]*0.2)]\n",
        "\n",
        "      if train_images.shape[1:] == (28, 28):\n",
        "        train_images_r = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
        "        test_images_r = test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
        "      else:\n",
        "        train_images_r, test_images_r = train_images, test_images\n",
        "\n",
        "      start = time.perf_counter()\n",
        "\n",
        "      # select approach:\n",
        "      if gpu:\n",
        "        with tf.device('/device:GPU:0'):\n",
        "          accuracy = call_func(train_images, train_labels, test_images, test_labels, train_images_r, test_images_r)\n",
        "      else:\n",
        "        accuracy = call_func(train_images, train_labels, test_images, test_labels, train_images_r, test_images_r)\n",
        "\n",
        "      # process data \n",
        "      if _ > 0 and _ != r-1:\n",
        "        batch[1] += accuracy \n",
        "        batch[2] += time.perf_counter() - start\n",
        "      elif _ == r-1:\n",
        "        batch[1] += accuracy\n",
        "        batch[2] += time.perf_counter() - start\n",
        "        batch[1], batch[2] = batch[1]/r, batch[2]/r\n",
        "      else:\n",
        "        batch.append(accuracy)\n",
        "        batch.append(time.perf_counter() - start)\n",
        "\n",
        "  print('')\n",
        "  latex_data.append(f'Appraoch: {func} with (batch, accuracy), (time, batch) {conv_to_latex(data_acc)}')\n",
        "  print(latex_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAQc5PjspXTy"
      },
      "source": [
        "for a in latex_data:\n",
        "  print(a)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
